<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2015-05-24" />

<title>重讀《An introduction to statistical learning》筆記 第四章</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<link rel="shortcut icon" href="favicon.ico" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Bei's Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="https://github.com/yishi">GitHub</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">重讀《An introduction to statistical learning》筆記 第四章</h1>
<h4 class="author"></h4>
<h4 class="date">2015-05-24</h4>

</div>


<p><strong>chapter 4 Classification</strong></p>
<ul>
<li>3 methods :</li>
</ul>
<ol style="list-style-type: decimal">
<li>logistic regression</li>
<li>linear discriminant analysis</li>
<li>K - nearest neighbors</li>
</ol>
<ul>
<li>other computer-intensive methods
<ul>
<li>generalized additive models</li>
<li>trees</li>
<li>random forests</li>
<li>boosting</li>
<li>support vector machine</li>
</ul></li>
<li>logistic regression</li>
</ul>
<p>邏輯回歸對Y屬於某個分類的概率進行建模。</p>
<p>下面是示例用數據集，Y變量：是否拖欠債務；解釋變量是income和balance。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-1.png" /></p>
<p>下圖左邊是使用線性回歸對Y變量分析，概率有負值，右邊是邏輯回歸進行建模，概率在0和1之間。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-2.png" /></p>
<p>線性回歸的形式對Ｙ屬於某類別的概率進行建模 <span class="math display">\[p(X) = \beta_0 + \beta_1X\]</span></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-3.png" /></p>
<p>使用邏輯斯形式，把概率值規範在0和1之間 <span class="math display">\[p(X) = \frac{e^{\beta_0 + \beta_1X}}{1+e^{\beta_0 + \beta_1X}}\]</span></p>
<p>方程變換后的模型： <span class="math display">\[\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}\]</span></p>
<p>方程左邊是個比值，數值偏向兩端。</p>
<p>優勢比，出現正面的概率比上出現反面的概率的比值,</p>
<p>如0.2/（1-0.2）=0.25</p>
<p>0.9/（1-0.9）=9</p>
<p>爲了讓比值平穩，對方程兩邊同時取對數，得到： <span class="math display">\[log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1X\]</span></p>
<p>估計回歸係數：最大似然法 maximum likelihood選擇能夠最大化下面的似然方程的係數值。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-4.png" /></p>
<p>最小平方法是最大似然法的特例。</p>
<p>最大似然法常用於擬合非線性模型。</p>
<p>邏輯斯回歸中也可以使用離散變量做解釋變量，使用虛擬變量的形式。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-5.png" /></p>
<p>在單變量邏輯回歸中，離散變量學生顯著，而且係數為正，表明學生容易拖欠信用卡。</p>
<p><strong>多變量邏輯回歸</strong> <span class="math display">\[log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p\]</span></p>
<p><span class="math display">\[p(X) = \frac{e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}{1+e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}\]</span></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-6.png" /></p>
<p>在多變量邏輯回歸中，離散變量學生顯著，係數為負，表明學生不易拖欠信用卡，與單變量邏輯回歸的結果剛好相反。</p>
<p>單變量回歸時考慮的是下圖左圖中的虛線，學生和非學生的整體拖欠水平，學生較高；</p>
<p>多變量中考慮的是下圖左圖中的實線，同樣的balance值，非學生的拖欠更高。</p>
<p>原因在于：學生因素和變量balance相關。</p>
<p><strong>因此，單變量回歸很危險，當單變量可能與其他變量相關時。</strong></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-7.png" /></p>
<ul>
<li>linear discriminant analysis</li>
</ul>
<p>使用于當Y變量有三個以上類別</p>
<p>使用貝葉斯理論分類 <span class="math display">\[Pr(Y=k|X =x) = \frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}\]</span></p>
<p>lda for p = 1</p>
<p>假定對於某個觀察值屬於第k個類別的解釋變量的密度函數是正態分佈或高斯分佈，分佈如下所示： <span class="math display">\[f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)\]</span></p>
<p>插入計算后驗概率的公式得到： <span class="math display">\[p_k(x) = \frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2)}\]</span></p>
<p><code>$\pi_k$</code>代表一個隨機選擇的觀察值屬於第k類別的先驗概率。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-8.png" /></p>
<p>LDA近似貝葉斯分類器，通過插入一些係數估計值， <span class="math display">\[\hat{\mu_k} = \frac{1}{n_k}\sum_{i:y_i=k}x_i\]</span></p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i - \hat{\mu}_k)^2\]</span></p>
<p><span class="math display">\[\hat{\pi}_k = n_k/n\]</span></p>
<p>加入計算后驗概率的公式，兩遍取對數，適當變換，得到discriminant functions， 他是x的線性方程。</p>
<p><span class="math display">\[\hat{\delta}_k(x) = x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + log(\hat{\pi}_k)\]</span></p>
<p>lda for p &gt; 1</p>
<p>假定解釋變量服從多變量高斯分佈，有均值和共同的協方差矩陣。</p>
<p><span class="math display">\[f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\]</span></p>
<p>求和符號代表協方差矩陣。</p>
<p><span class="math display">\[\delta_k(x) = x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log\pi_k\]</span></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-9.png" /></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-10.png" /></p>
<p>LDA嘗試近似貝葉斯分類器，取得最小的總體錯誤率。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-11.png" /></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-12.png" /></p>
<p>貝葉斯分類器使用臨界值0.5，有最小的總體錯誤率。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-13.png" /></p>
<p>Y軸 <strong>TPR(true positive rate) sensitivity recall power 1-type 2 error:某臨界點處，被正確識別的拖欠者的比例；</strong></p>
<p>X軸 <strong>FPR(false prositive rate) 1-specificity type1 error:在同一臨界點處，非拖欠者被誤判為拖欠者的比例。</strong></p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-14.png" /></p>
<ul>
<li>QDA (quadratic discriminant analysis)</li>
</ul>
<p>假定Y變量的每個類別有各自的協方差矩陣。</p>
<p><span class="math display">\[\delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + log\pi_k \\\\ 
= -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k + log\pi_k\]</span></p>
<p>選擇LDA還是QDA，基於偏差和方差之間的權衡。</p>
<p>如果有p個解釋變量，LDA需要估計p(p+1)/2個係數，QDA需要估計kp(p+1)/2個係數。</p>
<p>如果有相對少的訓練數據，使用LDA，因為降低方差是關鍵。</p>
<p>如果有大量的訓練數據，或者K個類別屬於共同的協方差矩陣站不住腳的時候，使用QDA。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-15.png" /></p>
<ul>
<li>分類模型比較
<ul>
<li>logistic regression</li>
<li>LDA</li>
<li>QDA</li>
<li>KNN</li>
</ul></li>
</ul>
<p>logistic regression和LDA比較近似；</p>
<p>都產生線性決策邊界；</p>
<p>不同在於擬合過程不同，前者使用最大似然，後者使用正態分佈估計的均值和方差來擬合模型。</p>
<p>KNN是完全非參數的，適用於決策邊界高度非線性。</p>
<p>QDA介於上述兩者之間。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-16.png" /></p>
<p>上邊的左圖：兩個解釋變量服從正態分佈，相互獨立。LDA運行的最好。</p>
<p>上邊的中圖：兩個解釋變量屬於正態分佈，相關係數-0.5。</p>
<p>上邊的右圖：兩個解釋變量服從t分佈，t分佈更偏向兩端，logistic regression 運行最好。</p>
<p><img src="images/2015-05-25-重讀-an-introduction-to-statistical-learning-筆記-第四章-17.png" /></p>
<p>上邊的左圖：服從正態分佈，在第一個類別中的解釋變量之間的相關性為0.5，第二個類別間為-0.5。QDA運行最好。</p>
<p>上邊的中圖：正態分佈，相互獨立，使用解釋變量的平方項和交互項做解釋變量，QDA運行最好，然後是KNN-CV。</p>
<p>上邊的右圖：複雜的非線性，KNN-CV運行最好。</p>
<ul>
<li>具體的R中的函數</li>
</ul>
<p>logistic regression</p>
<p><code>glm(, , family = binomial)</code> 詳見167頁</p>
<p>lda 詳見172頁</p>
<pre class="{r}"><code>library(MASS)
lda()</code></pre>
<p>qda 詳見174頁</p>
<pre class="{r}"><code>library(MASS)
qda()</code></pre>
<p>knn 詳見175頁</p>
<pre class="{r}"><code>library(class)
knn()</code></pre>
<p>knn建模之前的數據需要預處理，<strong>數據標準化</strong>，統一單位，使所有變量都均值為0，標準差為1。使用<code>scale()</code>。</p>
<p><strong>本章重點：</strong> + 邏輯回歸適用於Y有兩個類別，使用最大似然，產生線性決策邊界； + 而且，單變量邏輯回歸很危險，當單變量可能與其他變量相關時； + LDA適用於Y有三個以上類別時，使用正態分佈估計的均值和方差來擬合模型，產生線性決策邊界； + QDA，Y變量的每個類別有各自的協方差矩陣； + KNN是完全非參數的，適用於決策邊界高度非線性； + KNN建模之前的數據需要預處理，數據標準化。</p>
<p>备注：转移自新浪博客，截至2021年11月，原阅读数135，评论0个。</p>


  <!-- Link Gitalk 的支持文件  -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script src="md5.min.js"></script>
<div id="gitalk-container"></div>     
<script type="text/javascript">
    const gitalk = new Gitalk({
    // gitalk的主要参数
		clientID: '132a6ad2a350dd61828f',
		clientSecret: '15341d30534ecad1cceb3eca0311809f8b95136a',
		repo: 'blog_rmarkdown',
		owner: 'yishi',
		admin: ['yishi'],
		id: md5(window.location.pathname),// Ensure uniqueness and length less than 50
    distractionFreeMode: true  // Facebook-like distraction free mode
    
    });
    gitalk.render('gitalk-container');
</script> 
<!-- Gitalk end -->




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
